{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de84a707-c5e0-45af-a66a-66787b417872",
   "metadata": {},
   "source": [
    "# House Price Prediction With TensorFlow\n",
    "\n",
    "[![open_in_colab][colab_badge]][colab_notebook_link]\n",
    "<!-- [![open_in_binder][binder_badge]][binder_notebook_link] -->\n",
    "\n",
    "[colab_badge]: https://colab.research.google.com/assets/colab-badge.svg\n",
    "[colab_notebook_link]: https://colab.research.google.com/github/foursquare/fsq-studio-sdk-examples/blob/master/python-notebooks/08%20-%20Tensorflow_prediction.ipynb\n",
    "<!-- [binder_badge]: https://mybinder.org/badge_logo.svg\n",
    "[binder_notebook_link]: https://mybinder.org/v2/gh/foursquare/fsq-studio-sdk-examples/master?urlpath=lab/tree/python-notebooks/08%20-%20Tensorflow_prediction.ipynb -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c04f6f-6bdb-4cf6-8a4e-64639c262a74",
   "metadata": {},
   "source": [
    "This example demonstrates how the Studio Map SDK allows for more engaging exploratory data visualization, helping to simplify the process of building a machine learning model for predicting median house prices in California."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1cc8d4-69cb-412f-8727-233d26ad954f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241896e-9cdc-4143-ba2b-3fd303ad05f6",
   "metadata": {},
   "source": [
    "This notebook uses the following dependencies:\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn\n",
    "- scipy\n",
    "- seaborn\n",
    "- matplotlib\n",
    "- tensorflow\n",
    "\n",
    "If running this notebook in Binder, these dependencies should already be installed. If running in Colab, the next cell will install these dependencies. In another environment, you'll need to make sure these dependencies are available by running the following `pip` command in a shell.\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn scipy seaborn matplotlib tensorflow\n",
    "```\n",
    "\n",
    "This notebook was originally tested with the following package versions, but likely works with a broad range of versions:\n",
    "\n",
    "- pandas==1.3.2\n",
    "- numpy==1.19.5\n",
    "- scikit-learn==0.24.2\n",
    "- scipy==1.7.1\n",
    "- seaborn==0.11.2\n",
    "- matplotlib==3.4.3\n",
    "- tensorflow==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If in Colab, install this notebook's required dependencies\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install 'unfolded.map_sdk>=1.0' pandas numpy scikit-learn scipy seaborn matplotlib tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5160c-4f05-4dc1-9e6a-d044dd81947f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e176a7-8864-4ab6-bcc3-f5bf52c30f37",
   "metadata": {},
   "source": [
    "If you're running this notebook on Binder, you may see a notification like the following when running the next cell.\n",
    "```\n",
    "Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
    "Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
    "```\n",
    "This is expected behavior because the machines on which Binder is running are not equipped with GPUs. The notebook will still function fine, it will just run slightly slower than on a machine with a GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c477b-a2e3-4783-b4d7-6771126de879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from unfolded.map_sdk import create_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794e22f-8b77-4fe5-899e-1f8cb4604099",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135904b7-f8bf-4ffc-96c6-4860dd2a90d9",
   "metadata": {},
   "source": [
    "For this example we'll use data from Kaggle's [California Housing Prices](https://www.kaggle.com/camnugent/california-housing-prices) dataset under the CC0 license. This dataset contains information about the housing in each census area in California, as of the 1990 census."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367c4bc-2b9e-4442-a51b-3fb2a2afac78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_url = \"https://4sq-studio-public.s3.us-west-2.amazonaws.com/sdk/examples/sample-data/housing.csv\"\n",
    "housing = pd.read_csv(dataset_url)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55a1df-8e19-405c-a28a-f61eb42d8167",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58b8d8",
   "metadata": {},
   "source": [
    "First, let's take a look at the input data and try to visualize different aspects of them in a map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddbd66-1710-45f0-a4bc-fc11754720f1",
   "metadata": {},
   "source": [
    "### Population Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9157662-e55d-4b14-8b50-97b77873ca2f",
   "metadata": {},
   "source": [
    "In the next cell we'll create a map that clusters rows of the dataset according to population. Note that since the clustering happens within Foursquare Studio, the clusters are re-computed as you zoom in, allowing you to explore your data at various resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3acb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_in_CA = create_map()\n",
    "population_in_CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a persistent dataset ID that we can reference in both add_dataset and add_layer\n",
    "dataset_id = str(uuid4())\n",
    "\n",
    "population_in_CA.add_dataset(\n",
    "    id=dataset_id,\n",
    "    label=\"Population_in_CA\",\n",
    "    data=housing,\n",
    "    auto_create_layers=False,\n",
    ")\n",
    "\n",
    "population_in_CA.add_layer(\n",
    "    {\n",
    "        \"id\": \"population_CA\",\n",
    "        \"type\": \"cluster\",\n",
    "        \"label\": \"population in CA\",\n",
    "        \"data_id\": dataset_id,\n",
    "        \"fields\": {\"lat\": \"latitude\", \"lng\": \"longitude\"},\n",
    "        \"config\": {\n",
    "            \"visual_channels\": {\n",
    "                \"colorScale\": \"quantize\",\n",
    "                \"colorField\": {\"name\": \"population\", \"type\": \"real\"},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "population_in_CA.set_view(longitude=-119.417931, latitude=36.778259, zoom=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587cfaf-a1be-4940-9c60-d00c31203131",
   "metadata": {},
   "source": [
    "### Distances from housing areas to largest cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9f9a7",
   "metadata": {},
   "source": [
    "Next, we want to explore where the housing areas in our dataset are located in comparison to the largest cities in California. For example purposes, we'll take the five largest cities in California and compare our input data against these locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longitude-latitude pairs for large cities\n",
    "cities = {\n",
    "    \"Los Angeles\": (-118.244, 34.052),\n",
    "    \"San Diego\": (-117.165, 32.716),\n",
    "    \"San Jose\": (-121.895, 37.339),\n",
    "    \"San Francisco\": (-122.419, 37.775),\n",
    "    \"Fresno\": (-119.772, 36.748),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bcee73-45b5-4fd8-b668-a2618157a561",
   "metadata": {},
   "source": [
    "Next we need to find the closest city for each row in our data sample. First we'll define a couple functions to help compute the distance between cities and the city closest to a specific point. Then we'll apply these functions on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c99b5-3256-4739-a457-5c670850069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lng1, lat1, lng2, lat2):\n",
    "    \"\"\"Vectorized Haversine formula\n",
    "\n",
    "    Computes distances between two sets of points.\n",
    "\n",
    "    From: https://stackoverflow.com/a/51722117\n",
    "    \"\"\"\n",
    "    # approximate radius of earth in km\n",
    "    R = 6371.009\n",
    "\n",
    "    lat1 = lat1*np.pi/180.0\n",
    "    lng1 = np.deg2rad(lng1)\n",
    "    lat2 = np.deg2rad(lat2)\n",
    "    lng2 = np.deg2rad(lng2)\n",
    "\n",
    "    d = np.sin((lat2 - lat1)/2)**2 + np.cos(lat1)*np.cos(lat2) * np.sin((lng2 - lng1)/2)**2\n",
    "\n",
    "    return 2 * R * np.arcsin(np.sqrt(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c729046-6a42-4241-8fd7-6dac2264ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_city(lng_array, lat_array, cities):\n",
    "    \"\"\"Find the closest_city for each row in lng_array and lat_array input\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "\n",
    "    # Compute distance from each row of arrays to each of our city inputs\n",
    "    for city_name, coord in cities.items():\n",
    "        distances.append(distance(lng_array, lat_array, *coord))\n",
    "\n",
    "    # Convert this list of numpy arrays into a 2D numpy array\n",
    "    distances = np.array(distances)\n",
    "\n",
    "    # Find the shortest distance value for each row\n",
    "    shortest_distances = np.amin(distances, axis=0)\n",
    "\n",
    "    # Find the _index_ of the shortest distance for each row. Then use this value to\n",
    "    # lookup the longitude-latitude pair of the closest city\n",
    "    city_index = np.argmin(distances, axis=0)\n",
    "\n",
    "    # Create a 2D numpy array of location coordinates\n",
    "    # Then use the indexes from above to perform a lookup against the order of cities as\n",
    "    # input. (Note: this relies on the fact that in Python 3.6+ dictionaries are\n",
    "    # ordered)\n",
    "    input_coords = np.array(list(cities.values()))\n",
    "    closest_city_coords = input_coords[city_index]\n",
    "\n",
    "    # Return a 2D array with three columns:\n",
    "    # - Distance to closest city\n",
    "    # - Longitude of closest city\n",
    "    # - Latitude of closest city\n",
    "    return np.hstack((shortest_distances[:, np.newaxis], closest_city_coords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c6f5d-0cab-4881-9612-eacc8327f3bc",
   "metadata": {},
   "source": [
    "Then use the `closest_city` function on our data to create three new columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb61de-5a70-4cd9-86c4-3e9686cbf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[['closest_city_dist', 'closest_city_lng', 'closest_city_lat']] = closest_city(\n",
    "    housing['longitude'], housing['latitude'], cities\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0da2d",
   "metadata": {},
   "source": [
    "The map created in the next cell uses the new columns we computed above in relation to the largest cities in California:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_to_big_cities = create_map()\n",
    "distance_to_big_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_data_id = str(uuid4())\n",
    "\n",
    "distance_to_big_cities.add_dataset(\n",
    "    id=dist_data_id,\n",
    "    label=\"Distance to closest big city\",\n",
    "    data=housing,\n",
    "    auto_create_layers=False,\n",
    ")\n",
    "\n",
    "distance_to_big_cities.add_layer(\n",
    "    {\n",
    "        \"id\": \"closest_distance\",\n",
    "        \"type\": \"arc\",\n",
    "        \"data_id\": dist_data_id,\n",
    "        \"label\": \"distance to closest big city\",\n",
    "        \"fields\": {\n",
    "            \"lng0\": \"longitude\",\n",
    "            \"lat0\": \"latitude\",\n",
    "            \"lng1\": \"closest_city_lng\",\n",
    "            \"lat1\": \"closest_city_lat\",\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"vis_config\": {\"opacity\": 0.8, \"thickness\": 0.3},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "distance_to_big_cities.set_view(longitude=-119.417931, latitude=36.778259, zoom=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada3234-1d8e-4371-a913-956c009433af",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3191d",
   "metadata": {},
   "source": [
    "In this next section, we want to prepare our dataset to be used for training a TensorFlow model. First, we'll drop rows with null values, since they're quite rare in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c0961-c164-4515-adee-defbde33cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_null_rows = housing.isnull().any(axis=1).sum() / len(housing) * 100\n",
    "print(f'{pct_null_rows:.1f}% of rows have null values')\n",
    "\n",
    "housing = housing.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d3215-958d-480e-83dc-fae9869cc83d",
   "metadata": {},
   "source": [
    "In the model we're training, we want to predict the median house value of an area. Thus we split the columns from our dataset `housing` into a dataset `y` with the column `median_house_value` and a dataset `X` with all other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_column = ['median_house_value']\n",
    "other_columns = housing.columns.difference(predicted_column)\n",
    "\n",
    "X = housing.loc[:, other_columns]\n",
    "y = housing.loc[:, predicted_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cae51-20ff-4e24-8e3f-69f19359b3d0",
   "metadata": {},
   "source": [
    "Most of the columns in `X` are numeric, but one is not. `ocean_proximity` is of type `object`, which here is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f4570-fdae-4dc4-9daf-77374900b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc89ba2-caab-4829-aeb4-a48f9b7ca7a7",
   "metadata": {},
   "source": [
    "Looking closer, we see that `ocean_proximity` is a categorical string with only five values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386cfba-fa9a-4492-bb17-d31c03f6e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f6123-61ee-4b5d-88fd-b92b9c6829c5",
   "metadata": {},
   "source": [
    "In order to use this column in our numeric model, we call [`pandas.get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to create five new boolean columns. Each of these columns contains a `1` if the value of `ocean_proximity` is equal to the value that's now the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6441b-de6f-471f-a1e9-ff9eb8532fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(\n",
    "    data=X, columns=[\"ocean_proximity\"], prefix=[\"ocean_proximity\"], drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40993536-76e3-4503-af0c-ef26f2676bcc",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7673a78-81da-4bf4-bbca-ba15b2135597",
   "metadata": {
    "tags": []
   },
   "source": [
    "In line with standard machine learning practice, we split our dataset into training, validation and test sets. We first take out 20% of our full dataset to use for testing the model after training. Then of the remaining 80%, we take out 75% to use for training the model and 25% to use for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe20ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing training data into test, validation and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=1\n",
    ")\n",
    "\n",
    "# We save a copy of our test data to use after model prediction\n",
    "start_values = X_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d9cf3",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd6962-258d-4c18-bb8a-270d03f6aa9f",
   "metadata": {},
   "source": [
    "We use standard scaling with mean and standard deviation from our training dataset to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e62a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f9308-6d91-475a-83c1-dfbbef814272",
   "metadata": {},
   "source": [
    "## Price Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812992b",
   "metadata": {},
   "source": [
    "Next we specify the parameters for the TensorFlow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f697f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a Sequential model from Keras\n",
    "# https://keras.io/api/models/sequential/\n",
    "model = Sequential()\n",
    "\n",
    "# Each column from X is an input feature into our model.\n",
    "number_of_features = len(X.columns)\n",
    "\n",
    "# input Layer\n",
    "model.add(Dense(number_of_features, activation=\"relu\", input_dim=number_of_features))\n",
    "\n",
    "# hidden Layer\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "\n",
    "# output Layer\n",
    "model.add(Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6cb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\", \"mae\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d0bd8-e34d-427e-b150-53a33a5357f4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f24ea",
   "metadata": {},
   "source": [
    "Next we begin model training. Model training can take a long time; the higher the number of epochs, the better the model will be fit, but the longer training will take. Here we default to only 10 epochs because the focus of this notebook is integration with Foursquare Studio, not the machine learning itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b159541-7176-47c5-9fa7-3f634273e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "# Or uncomment the following line if you're happy to wait longer for a better model fit.\n",
    "# EPOCHS = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a020f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train.to_numpy(),\n",
    "    batch_size=10,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=(X_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8bdd26-7a6a-4564-af3b-c2c0cac3f5b2",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb6304",
   "metadata": {},
   "source": [
    "Next we want to find out how well the model was trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4076a3a4-c154-43ea-a833-6d91ce123bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "loss_train = history.history[\"loss\"]\n",
    "loss_val = history.history[\"val_loss\"]\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(epochs, loss_train, \"g\", label=\"Training loss\")\n",
    "plt.plot(epochs, loss_val, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e5e9e",
   "metadata": {},
   "source": [
    "In the above chart we can see that the training loss and validation loss are quite close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3474c70",
   "metadata": {},
   "source": [
    "Now we can use our trained model to predict home prices on the _test_ data, which was not used in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c742425",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cbc56",
   "metadata": {},
   "source": [
    "We can see that loss function value on the test data is similar to the loss value on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18756b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87db7d6-d1a3-420d-9ab2-e6e3223a4aeb",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea00a7f",
   "metadata": {},
   "source": [
    "Let's now visualize our housing price predictions using Foursquare Studio. Here we create a dataframe with predicted values obtained from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = start_values.loc[:, ['longitude', 'latitude']]\n",
    "predict_data[\"price\"] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb728127-0c64-4b4e-ab6b-71d451f33513",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94536742",
   "metadata": {},
   "source": [
    "The map we create in the next cell depicts the prices we've predicted for houses in each census area in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predict_prices = create_map()\n",
    "housing_predict_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15e7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "price_data_id = str(uuid4())\n",
    "\n",
    "housing_predict_prices.add_dataset(\n",
    "    id=price_data_id,\n",
    "    label=\"Predict housing prices in CA\",\n",
    "    data=predict_data,\n",
    "    auto_create_layers=False,\n",
    ")\n",
    "\n",
    "housing_predict_prices.add_layer(\n",
    "    {\n",
    "        \"id\": \"housing_prices\",\n",
    "        \"type\": \"hexagon\",\n",
    "        \"label\": \"housing prices\",\n",
    "        \"data_id\": price_data_id,\n",
    "        \"fields\": {\"lat\": \"latitude\", \"lng\": \"longitude\"},\n",
    "        \"config\": {\n",
    "            \"visual_channels\": {\n",
    "                \"color_scale\": \"quantize\",\n",
    "                \"color_field\": {\"name\": \"price\", \"type\": \"real\"},\n",
    "            },\n",
    "            \"vis_config\": {\n",
    "                \"colorRange\": {\n",
    "                    \"colors\": [\n",
    "                        \"#E6F598\",\n",
    "                        \"#ABDDA4\",\n",
    "                        \"#66C2A5\",\n",
    "                        \"#3288BD\",\n",
    "                        \"#5E4FA2\",\n",
    "                        \"#9E0142\",\n",
    "                        \"#D53E4F\",\n",
    "                        \"#F46D43\",\n",
    "                        \"#FDAE61\",\n",
    "                        \"#FEE08B\",\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "housing_predict_prices.set_view(longitude=-119.417931, latitude=36.6, zoom=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b897ea-3a74-47bf-bf56-e006e4e0bae8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clustering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae748c09-3336-4b5d-9d7b-f6b5cd8a3674",
   "metadata": {},
   "source": [
    "We'll now cluster the predicted data by price levels using the KMeans algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "km = KMeans(n_clusters=k, init=\"k-means++\")\n",
    "X = predict_data.loc[:, [\"latitude\", \"longitude\", \"price\"]]\n",
    "\n",
    "# Run clustering and add to prediction dataset dataset\n",
    "predict_data[\"cluster\"] = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc816d7f-67c1-4b83-abbb-22784a511164",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e502b33",
   "metadata": {},
   "source": [
    "Let's show the price clusters in a chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(\n",
    "    x=\"latitude\",\n",
    "    y=\"longitude\",\n",
    "    data=predict_data,\n",
    "    palette=sns.color_palette(\"bright\", k),\n",
    "    hue=\"cluster\",\n",
    "    size_order=[1, 0],\n",
    "    ax=ax,\n",
    ").set_title(f\"Clustering (k={k})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619dcb6a",
   "metadata": {},
   "source": [
    "The next map shows the same clusters in a geographic context. Here we can see that house prices are highest for areas close to the largest cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c865a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_map_prices = create_map()\n",
    "unfolded_map_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2897425",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_dataset_id = str(uuid4())\n",
    "\n",
    "unfolded_map_prices.add_dataset(\n",
    "    id=prices_dataset_id,\n",
    "    label=\"Prices\",\n",
    "    data=predict_data,\n",
    "    auto_create_layers=False,\n",
    ")\n",
    "\n",
    "unfolded_map_prices.add_layer(\n",
    "    {\n",
    "        \"id\": \"prices_CA\",\n",
    "        \"type\": \"point\",\n",
    "        \"data_id\": prices_dataset_id,\n",
    "        \"label\": \"clustering of prices\",\n",
    "        \"fields\": {\"lat\": \"latitude\", \"lng\": \"longitude\"},\n",
    "        \"config\": {\n",
    "            \"visual_channels\": {\n",
    "                \"colorScale\": \"quantize\",\n",
    "                \"colorField\": {\"name\": \"cluster\", \"type\": \"real\"},\n",
    "            },\n",
    "            \"vis_config\": {\n",
    "                \"colorRange\": {\n",
    "                    \"colors\": [\"#7FFFD4\", \"#8A2BE2\", \"#00008B\", \"#FF8C00\", \"#FF1493\"]\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "unfolded_map_prices.set_view(longitude=-119.417931, latitude=36.778259, zoom=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8664e04-7bb2-417b-8626-c774b4dbd69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
