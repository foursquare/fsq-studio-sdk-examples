{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe9894e-c6c5-48d1-ad09-b48a32eb0b86",
   "metadata": {},
   "source": [
    "# Unfolded SDK Machine Learning Demo (LA Bike Share)\n",
    "\n",
    "[![open_in_colab][colab_badge]][colab_notebook_link]\n",
    "[![open_in_binder][binder_badge]][binder_notebook_link]\n",
    "\n",
    "[colab_badge]: https://colab.research.google.com/assets/colab-badge.svg\n",
    "[colab_notebook_link]: https://colab.research.google.com/github/foursquare/unfolded-sdk-examples/blob/master/notebooks/11%20-%20Pytorch%20Trip%20Duration.ipynb\n",
    "[binder_badge]: https://mybinder.org/badge_logo.svg\n",
    "[binder_notebook_link]: https://mybinder.org/v2/gh/foursquare/unfolded-sdk-examples/master?urlpath=lab/tree/notebooks/11%20-%20Pytorch%20Trip%20Duration.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bc768-9642-49e0-92b3-86c71524fe6d",
   "metadata": {},
   "source": [
    "For this demo [data](https://www.kaggle.com/cityofLA/los-angeles-metro-bike-share-trip-data) was taken from Kaggle. It contains information about bike share trips throughout a year in Los Angeles. The task is to predict bike trip duration as it may be useful to know when a bike will be returned. Prediction is made using starting share point location, time and some other info.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b2a33-f445-4a46-9979-613b13b5cd4e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef774e-7f31-4a17-9936-3086a3c73f2a",
   "metadata": {},
   "source": [
    "This notebook uses the following dependencies:\n",
    "\n",
    "- `xarray`\n",
    "- `dask`\n",
    "- `netCDF4`\n",
    "- `bottleneck`\n",
    "- `tqdm`\n",
    "- `dask-ml`\n",
    "- `pandas`\n",
    "- `seaborn`\n",
    "- `matplotlib`\n",
    "- `scikit-learn`\n",
    "- `category_encoders`\n",
    "- `torch`\n",
    "- `missingno`\n",
    "- `unfolded.map-sdk`\n",
    "- `unfolded.data-sdk`\n",
    "\n",
    "If running this notebook in Binder, these dependencies should already be installed. If running in Colab, the next cell will install these dependencies. In another environment, you'll need to make sure these dependencies are available by running the following `pip` command in a shell.\n",
    "\n",
    "```bash\n",
    "pip install xarray dask netCDF4 bottleneck tqdm dask-ml pandas seaborn matplotlib scikit-learn category_encoders torch missingno unfolded.map-sdk unfolded.data-sdk\n",
    "```\n",
    "\n",
    "This notebook was originally tested with the following package versions, but likely works with a broad range of versions:\n",
    "\n",
    "- xarray==0.19.0 \n",
    "- dask==2021.09.1 \n",
    "- netCDF4==1.5.7 \n",
    "- bottleneck==1.3.2 \n",
    "- tqdm==4.62.3 \n",
    "- dask-ml==1.9.0 \n",
    "- pandas==1.3.3 \n",
    "- seaborn==0.11.2 \n",
    "- matplotlib==3.4.3 \n",
    "- scikit-learn==1.0 \n",
    "- category_encoders==2.2.2\n",
    "- torch==1.9.1 \n",
    "- missingno==0.5.0 \n",
    "- unfolded.map-sdk==0.5.0\n",
    "- unfolded.data-sdk==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If in Colab, install this notebook's required dependencies\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install 'unfolded.map_sdk>=1.0' xarray dask netCDF4 bottleneck tqdm dask-ml pandas seaborn matplotlib scikit-learn category_encoders torch missingno unfolded.data-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcf934-6c0c-46e5-91ea-dd69a1250df3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c271546-9a7e-41e6-9974-bfbb24879a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import category_encoders as ce\n",
    "import missingno as msno\n",
    "from uuid import uuid4\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from unfolded.map_sdk import create_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c5c8a-116b-40f8-828f-55cb70e4d38b",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53d09c-3e64-41de-a566-e89de7bb9c62",
   "metadata": {},
   "source": [
    "Load <code>.nc</code> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebbb1b6-92c8-4e59-be90-5e89d910de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url, path):\n",
    "    if os.path.isfile(path):\n",
    "        pass\n",
    "    else:\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n",
    "        block_size = 1048576  # 1 Megabyte\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "        with open(path, \"wb\") as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "\n",
    "dataset_url = \"https://actionengine-public.s3.us-east-2.amazonaws.com/metro-bike-share-trip-data.nc\"\n",
    "dataset_path = \"metro-bike-share-trip-data-downloaded.nc\"\n",
    "load_data(dataset_url, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94d475-8c74-40ea-afc3-c7e9970e27da",
   "metadata": {},
   "source": [
    "## Data Clean-up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b85ce0-92d2-44cf-8b82-37869e2c087e",
   "metadata": {},
   "source": [
    "Open dataset and convert it to dask format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd0026-ecc0-45e6-adbf-07531ed32db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"metro-bike-share-trip-data-downloaded.nc\", chunks={\"Start Time\": 135000})\n",
    "ddf = ds.to_dask_dataframe(set_index=True)\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebda2e-ed56-428e-a922-662c27467e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset has \", ddf.shape[0].compute(), \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221954d0-0fb0-4d34-a24b-2978880a96c8",
   "metadata": {},
   "source": [
    "Look at how many NaNs are in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e0ec6-1de3-4ace-8da8-540fb067421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(ddf.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac979656-9f8b-4c2f-b34c-dbf4f2bcfb66",
   "metadata": {},
   "source": [
    "Delete raws without necessary information: starting or ending point latutude and longitude, start time, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420712d8-33a3-45f3-93e3-d375f53a1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.dropna(\n",
    "    subset=[\n",
    "        \"Starting Station ID\",\n",
    "        \"Starting Station Latitude\",\n",
    "        \"Starting Station Longitude\",\n",
    "        \"Ending Station Latitude\",\n",
    "        \"Ending Station Longitude\",\n",
    "        \"Start Time\",\n",
    "        \"Precinct Boundaries\",\n",
    "        \"Census Tracts\",\n",
    "        \"Plan Duration\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363b809-fc76-45f6-868a-7fed191e80cb",
   "metadata": {},
   "source": [
    "Delete rows with 0 in <b>Starting Station Latitude</b> column and fill all NaNs with zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ad615-50cf-4bbc-b681-3b53d60290e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf[(ddf[\"Starting Station Latitude\"] != 0)]\n",
    "ddf = ddf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a65a3-c964-4a77-b9f2-4e0b929bc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset has \", ddf.shape[0].compute(), \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01649bc-2c33-41cc-9fab-388a16019f03",
   "metadata": {},
   "source": [
    "### Trip Duration Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e1f30-3cd2-41a6-a1f7-237824ab6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = sns.histplot(data=ddf[\"Duration\"], bins=100)\n",
    "hist.set(xlabel=\"Duration\")\n",
    "hist.set_title(\"Trip duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80706a-b72a-42b6-8406-8a5320d61ac1",
   "metadata": {},
   "source": [
    "Remove outliers from <b>Duration</b> column. The IQR (Interquartile Range) method is used. The lower border is Q1-1.5\\*IQR and the upper border is Q3+1.5\\*IQR <br>\n",
    "[To read more](https://en.wikipedia.org/wiki/Interquartile_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a2eda-64f7-4acd-a629-f3a3ec803436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_borders(data):\n",
    "    q25, q75 = np.percentile(data, 25), np.percentile(data, 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * 1.5\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "lower, upper = outlier_borders(ddf[\"Duration\"])\n",
    "no_outliers = [x for x in ddf[\"Duration\"] if x > lower and x < upper]\n",
    "ddf = ddf[(ddf[\"Duration\"] < upper)]\n",
    "hist = sns.histplot(data=no_outliers, bins=20)\n",
    "hist.set(xlabel=\"Duration\")\n",
    "hist.set_title(\"Trip duration with no outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99177275-2452-4f09-bb96-de36a7f1d147",
   "metadata": {},
   "source": [
    "Manually remove two outlier records (to not remove useful dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962a671-0e1a-4aab-92cc-1544b031095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf[(ddf[\"Starting Station Longitude\"] > -118.3)]\n",
    "\n",
    "print(\"Dataset has \", ddf.shape[0].compute(), \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61775bae-d705-494b-8809-1dfd9a83167d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7eaed9-fd56-4df4-b99b-cd6714bfdbd1",
   "metadata": {},
   "source": [
    "### Starting Points Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb21554-d0e4-4ffb-84da-9a6945c0b2f2",
   "metadata": {},
   "source": [
    "Points on a map represent starting stations location. The lighter a point, the more trips started from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6908092",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_map = create_map()\n",
    "stations_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf267c64-af67-44d9-a20b-2b4a3bde57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make this plot open in 3D automatically and add height to hexagons\n",
    "\n",
    "ddf[\"Starting Station Latitude\"] = ddf[\"Starting Station Latitude\"].compute().round(6)\n",
    "ddf[\"Starting Station Longitude\"] = ddf[\"Starting Station Longitude\"].compute().round(6)\n",
    "ddf[\"Ending Station Latitude\"] = ddf[\"Ending Station Latitude\"].compute().round(6)\n",
    "ddf[\"Ending Station Longitude\"] = ddf[\"Ending Station Longitude\"].compute().round(6)\n",
    "\n",
    "start_stations = ddf[\n",
    "    [\"Starting Station Latitude\", \"Starting Station Longitude\"]\n",
    "].compute()\n",
    "\n",
    "start_stations = start_stations.groupby(\n",
    "    [\"Starting Station Latitude\", \"Starting Station Longitude\"]\n",
    ").size()\n",
    "start_stations = pd.DataFrame(start_stations.reset_index())\n",
    "start_stations.rename(columns={0: \"count\"}, inplace=True)\n",
    "\n",
    "stations_dataset_id = str(uuid4())\n",
    "\n",
    "stations_map.add_dataset(\n",
    "    id=stations_dataset_id, \n",
    "    label=\"Stations dataset\", \n",
    "    data=start_stations,\n",
    "    auto_create_layers=False,\n",
    ")\n",
    "\n",
    "stations_map.add_layer(\n",
    "    {\n",
    "        \"id\": \"Starting points\",\n",
    "        \"type\": \"hexagon\",\n",
    "        \"label\": \"Starting points\",\n",
    "        \"data_id\": stations_dataset_id,\n",
    "        \"fields\": {\n",
    "            \"lat\": \"Starting Station Latitude\",\n",
    "            \"lng\": \"Starting Station Longitude\",\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"visConfig\": {\"worldUnitSize\": 0.1,},\n",
    "            \"visualChannels\": {\n",
    "                \"colorField\": {\"name\": \"count\", \"type\": \"int\"},\n",
    "                \"colorScale\": \"quantize\",\n",
    "            }\n",
    "        },\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "stations_map.set_view(longitude=-118.25, latitude=34.04, zoom=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9098c7-8837-4dc5-9c6d-71dcd5b94094",
   "metadata": {},
   "source": [
    "### Routes Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06d767-8173-4629-a99c-1e5c5da50064",
   "metadata": {},
   "source": [
    "Arcs on a map connect starting and ending point of a trip. The lighter an arc, the more trips are between points it connects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f97fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_map = create_map()\n",
    "routes_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffca500-0b66-49e6-82a7-20be9ad638a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = ddf[\n",
    "    [\n",
    "        \"Starting Station Latitude\",\n",
    "        \"Starting Station Longitude\",\n",
    "        \"Ending Station Latitude\",\n",
    "        \"Ending Station Longitude\",\n",
    "    ]\n",
    "].compute()\n",
    "routes = routes.groupby(\n",
    "    [\n",
    "        \"Starting Station Latitude\",\n",
    "        \"Starting Station Longitude\",\n",
    "        \"Ending Station Latitude\",\n",
    "        \"Ending Station Longitude\",\n",
    "    ]\n",
    ").size()\n",
    "routes = pd.DataFrame(routes.reset_index())\n",
    "routes.rename(columns={0: \"count\"}, inplace=True)\n",
    "\n",
    "routes_dataset_id = str(uuid4())\n",
    "\n",
    "routes_map.add_dataset(\n",
    "    id=routes_dataset_id,\n",
    "    label=\"Routes dataset\",\n",
    "    data=routes,\n",
    "    auto_create_layers=False,\n",
    ")\n",
    "\n",
    "routes_map.add_layer(\n",
    "    {\n",
    "        \"id\": \"Routes\",\n",
    "        \"type\": \"arc\",\n",
    "        \"label\": \"Routes\",\n",
    "        \"data_id\": routes_dataset_id,\n",
    "        \"fields\": {\n",
    "            \"lat0\": \"Starting Station Latitude\",\n",
    "            \"lng0\": \"Starting Station Longitude\",\n",
    "            \"lat1\": \"Ending Station Latitude\",\n",
    "            \"lng1\": \"Ending Station Longitude\",\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"visual_channels\" : {\n",
    "                \"colorScale\": \"quantile\",\n",
    "                \"colorField\": {\"name\": \"count\", \"type\": \"int\"},\n",
    "            },\n",
    "            \"vis_config\": {\n",
    "                \"opacity\": 0.8,\n",
    "                \"thickness\": 0.3,\n",
    "                \"colorRange\": {\n",
    "                    \"colors\": [\n",
    "                        \"#5A1846\",\n",
    "                        \"#900C3F\",\n",
    "                        \"#C70039\",\n",
    "                        \"#E3611C\",\n",
    "                        \"#F1920E\",\n",
    "                        \"#FFC300\",\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "routes_map.set_view(longitude=-118.25, latitude=34.04, zoom=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca404005-9d4c-4127-9854-0513ad10c9b4",
   "metadata": {},
   "source": [
    "### Visualization of Category Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c63433-566c-4c5c-bc53-909fa1b9a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_category_stats = ddf[\"Trip Route Category\"].value_counts().compute()\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:5]\n",
    "plt.title(\"Trip Route Category\")\n",
    "plt.pie(\n",
    "    trip_category_stats.values,\n",
    "    labels=trip_category_stats.index,\n",
    "    colors=colors,\n",
    "    autopct=\"%.0f%%\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8440600-20d4-40e8-8ae5-47cf7aa2aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_category_stats = ddf[\"Passholder Type\"].value_counts().compute()\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:5]\n",
    "plt.title(\"Passholder Type\")\n",
    "plt.pie(\n",
    "    plan_category_stats.values,\n",
    "    labels=plan_category_stats.index,\n",
    "    colors=colors,\n",
    "    autopct=\"%.0f%%\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0354bae-2965-499d-b0ef-bce05aec5dd5",
   "metadata": {},
   "source": [
    "As shown in figures above, most people prefer to take one way trips and get a monthly pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683beb1-e437-4294-b1c8-7c4097b3d6b4",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf39659-3cca-4f44-a3f8-51f2bdde00f3",
   "metadata": {},
   "source": [
    "### Preprocess Datetime Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26ffdc-fccd-44f1-b0aa-903aea9f614e",
   "metadata": {},
   "source": [
    "All data has to be turned into numerical form, so datetime feature column <b>Start Time</b> is decomposed into several feature columns (<b>Year</b>, <b>Month</b>, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a75db-9f29-4e8b-a7d0-c30c8e51fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"Start Time\"] = pd.to_datetime(ddf[\"Start Time\"].compute())\n",
    "\n",
    "\n",
    "def extract_time_features(timestamp):\n",
    "    return [\n",
    "        timestamp.year,\n",
    "        timestamp.month,\n",
    "        timestamp.day,\n",
    "        timestamp.hour,\n",
    "        timestamp.minute,\n",
    "    ]\n",
    "\n",
    "\n",
    "ddf_to_concat = ddf.compute().reset_index(drop=True)\n",
    "time_features_ddf = ddf_to_concat[\"Start Time\"].apply(extract_time_features)\n",
    "time_features_df = pd.DataFrame(\n",
    "    time_features_ddf.tolist(), columns=[\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]\n",
    ")\n",
    "df_with_time_features = pd.concat([ddf_to_concat, time_features_df], axis=1)\n",
    "df_with_time_features = df_with_time_features.drop(columns=[\"Start Time\"])\n",
    "\n",
    "df_with_time_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16bfa82-1b35-4a88-ab81-fc47a274f1d0",
   "metadata": {},
   "source": [
    "### Split dataset on Train, Validation and Test Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24c901-6ae5-4290-a1b3-9a91b5a746ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_with_time_features[\"Duration\"]\n",
    "X = df_with_time_features.drop(\n",
    "    columns=[\"Duration\", \"Ending Station Latitude\", \"Ending Station Longitude\"]\n",
    ")\n",
    "\n",
    "X_cut, X_test, y_cut, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_cut, y_cut, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a09ac7-4897-4b56-82ab-6655f188d639",
   "metadata": {},
   "source": [
    "### Apply Count Encoder to Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613bd33-bb8e-40b6-9b47-0ca90ef5097a",
   "metadata": {},
   "source": [
    "As already mentioned, all data has to be turned into numerical form, including categorical variables. To do this, count encoding method was chosen. It replaces each categorical value in a column with the number proportional to the number of times this value appears in this column. <br>\n",
    "[To read more](http://contrib.scikit-learn.org/category_encoders/count.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba24e06-e1b3-47bc-8a9b-53cc9f71726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_count_encoder = [\n",
    "    \"Passholder Type\",\n",
    "    \"Trip Route Category\",\n",
    "    \"Starting Station ID\",\n",
    "    \"Plan Duration\",\n",
    "    \"Neighborhood Councils (Certified)\",\n",
    "    \"Council Districts\",\n",
    "    \"Zip Codes\",\n",
    "    \"Precinct Boundaries\",\n",
    "    \"Census Tracts\",\n",
    "]\n",
    "\n",
    "count_encoder = ce.CountEncoder(\n",
    "    cols=columns_for_count_encoder, return_df=True, normalize=True\n",
    ")\n",
    "\n",
    "count_encoder = count_encoder.fit(X_train)\n",
    "\n",
    "X_train = count_encoder.transform(X_train)\n",
    "X_val = count_encoder.transform(X_val)\n",
    "X_test = count_encoder.transform(X_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4cfb2-e207-4ad2-b208-c788ed06dd0d",
   "metadata": {},
   "source": [
    "### Apply Scaling to Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d6b90-6eb1-424e-8d73-ab9dd8365262",
   "metadata": {},
   "source": [
    "For features with values in different range normalization should be applied to make all ranges the same. Instead, features with larger range would influence the result more. Here MinMax Scaler was used to make range of all variables between 0 and 1.<br>\n",
    "[To read more](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b1f76-e6e8-4db1-8afc-13f8b8e7cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_normalize = [\n",
    "    \"Starting Station Latitude\",\n",
    "    \"Starting Station Longitude\",\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"Day\",\n",
    "    \"Hour\",\n",
    "    \"Minute\",\n",
    "]\n",
    "\n",
    "\n",
    "normalizer = MinMaxScaler()\n",
    "normalizer = normalizer.fit(X_train[columns_to_normalize])\n",
    "X_train[columns_to_normalize] = normalizer.transform(X_train[columns_to_normalize])\n",
    "X_val[columns_to_normalize] = normalizer.transform(X_val[columns_to_normalize])\n",
    "X_test[columns_to_normalize] = normalizer.transform(X_test[columns_to_normalize])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1028aa3-0411-431c-860e-5e25ac840a56",
   "metadata": {},
   "source": [
    "### Apply Log and Scaling to the Target Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fc1a9-b66b-47ef-a88b-e8934c7e642a",
   "metadata": {},
   "source": [
    "Neural networks need variables deviations to be standard, so log function was applied to target variable along with MinMax Scaler<br>\n",
    "[To read more](https://en.wikipedia.org/wiki/Data_transformation_(statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876cb00-b2b7-4b15-b7b8-587d625578e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = sns.histplot(data=y_train, bins=20)\n",
    "hist.set(xlabel=\"Duration\")\n",
    "hist.set_title(\"Trip duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032a1d0-4275-4466-9541-24b50fd6c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.apply(np.log)\n",
    "y_val = y_val.apply(np.log)\n",
    "y_test = y_test.apply(np.log)\n",
    "\n",
    "hist = sns.histplot(data=y_train, bins=20)\n",
    "hist.set(xlabel=\"log(Duration)\")\n",
    "hist.set_title(\"Trip duration log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292be2f-5fd5-46b5-9d50-29ce361fbf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_y = MinMaxScaler()\n",
    "normalizer_y = normalizer_y.fit(y_train.to_numpy().reshape(-1, 1))\n",
    "y_train = normalizer_y.transform(y_train.to_numpy().reshape(-1, 1)).squeeze()\n",
    "y_val = normalizer_y.transform(y_val.to_numpy().reshape(-1, 1)).squeeze()\n",
    "y_test = normalizer_y.transform(y_test.to_numpy().reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e41b43-f4c8-4a07-8e68-ae52260be731",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a8ded-dc20-4e04-aa6a-6d9ee6b9f8d9",
   "metadata": {},
   "source": [
    "The model has 5 linear layers with ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0939b6-0882-4f8a-add3-f1ac1c2a7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_feature, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d6b0b-c00a-4d0c-a664-c4a6a991ea59",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b378e8e-554d-4ba2-9c2b-27d438788410",
   "metadata": {},
   "source": [
    "<code>USE_PRETRAINED_MODEL</code> should be set to <code>True</code> if you want to download and use a pretrained model. If you want to train your model, please set it to <code>False</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6dfa07-9ee7-4d38-95cf-6d023047a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2de112-aa40-4296-a281-8ac6372843fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def fit():\n",
    "    start = time.time()\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    num_batches_train = (n - 1) // batch_size + 1\n",
    "    num_batches_val = (X_val_t.shape[0] - 1) // batch_size + 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        for i in range(num_batches_train):\n",
    "            optimizer.zero_grad()\n",
    "            random_indexes = random.sample(range(0, X_train_t.shape[0] - 1), batch_size)\n",
    "            xb = X_train_t[random_indexes].float()\n",
    "            yb = y_train_t[random_indexes].float().reshape(-1, 1)\n",
    "            pred = net(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        for i in range(num_batches_val):\n",
    "            start_i = i * batch_size\n",
    "            end_i = start_i + batch_size\n",
    "            xb = X_val_t[start_i:end_i].float()\n",
    "            yb = y_val_t[start_i:end_i].float().reshape(-1, 1)\n",
    "            pred = net(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            epoch_val_loss += loss.item()\n",
    "\n",
    "        epoch_train_loss /= num_batches_train\n",
    "        epoch_val_loss /= num_batches_val\n",
    "        losses_train.append(epoch_train_loss)\n",
    "        losses_val.append(epoch_val_loss)\n",
    "        print(\n",
    "            \"Epoch \"\n",
    "            + str(epoch)\n",
    "            + \" %s (%d %d%%) Train loss: %.4f Validation loss: %.4f\"\n",
    "            % (\n",
    "                timeSince(start),\n",
    "                epoch,\n",
    "                epoch / epochs * 100,\n",
    "                epoch_train_loss,\n",
    "                epoch_val_loss,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses_train, losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebe596-66a9-48ff-b9ce-ab3a9fa5b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, y_train_t, X_val_t, y_val_t, X_test_t, y_test_t = map(\n",
    "    torch.tensor,\n",
    "    (X_train.to_numpy(), y_train, X_val.to_numpy(), y_val, X_test.to_numpy(), y_test),\n",
    ")\n",
    "\n",
    "if USE_PRETRAINED_MODEL:\n",
    "    model_url = \"https://actionengine-public.s3.us-east-2.amazonaws.com/model\"\n",
    "    model_path = \"model\"\n",
    "    load_data(model_url, model_path)\n",
    "    net = torch.load(\"model\")\n",
    "    net.eval()\n",
    "else:\n",
    "    net = Net(n_feature=16, n_output=1)  # define the network\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    loss_func = nn.MSELoss()  # this is for regression mean squared loss\n",
    "    batch_size = 20\n",
    "    n = X_train.shape[0]\n",
    "    epochs = 100\n",
    "\n",
    "    losses_train, losses_val = fit()\n",
    "    print(\n",
    "        \"Min train loss: \", min(losses_train), \" Min validation loss: \", min(losses_val)\n",
    "    )\n",
    "\n",
    "    torch.save(net, \"model\")\n",
    "    # show loss graph\n",
    "    loss_data = pd.DataFrame(\n",
    "        {\n",
    "            \"Epochs\": [j for j in range(0, epochs)],\n",
    "            \"Train loss\": losses_train,\n",
    "            \"Validation loss\": losses_val,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=\"Epochs\", y=\"value\", hue=\"variable\", data=pd.melt(loss_data, [\"Epochs\"])\n",
    "    ).set_title(\"Training process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae954637-5fa7-4191-99a1-fa8a32e65bd5",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25571d-be6c-43ce-bdb7-f8575ba408fa",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1dc541-c2ea-47de-ae3a-fe703884b1d5",
   "metadata": {},
   "source": [
    "[MSE](https://en.wikipedia.org/wiki/Mean_squared_error) and [MAE](https://en.wikipedia.org/wiki/Mean_absolute_error) metrics for normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61795c-b0ec-444d-9b4b-82266db4e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = X_test_t.shape[0]\n",
    "mse_metric = torch.nn.MSELoss()\n",
    "mae_metric = torch.nn.L1Loss()\n",
    "preds = []\n",
    "mse = 0\n",
    "mae = 0\n",
    "\n",
    "for i in range(test_len):\n",
    "    xb = X_test_t[i].float()\n",
    "    yb = y_test_t[i].float().reshape(1)\n",
    "    pred = net(xb)\n",
    "    preds.append(pred.item())\n",
    "    mse += mse_metric(pred, yb).item()\n",
    "    mae += mae_metric(pred, yb).item()\n",
    "\n",
    "mse /= test_len\n",
    "mae /= test_len\n",
    "\n",
    "print(\"MSE: \", mse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ddff63-b387-40d9-a0b2-8d3ce1ad63f3",
   "metadata": {},
   "source": [
    "MAE and [MAPE](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) metrics for original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae18fa-a89c-43a9-9a0c-543f02f3bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_y = normalizer_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_exp = np.exp(unnormalized_y)\n",
    "\n",
    "unnormalized_preds = normalizer_y.inverse_transform(np.array(preds).reshape(-1, 1))\n",
    "preds_exp = np.exp(unnormalized_preds)\n",
    "\n",
    "print(\"MAE: \", mean_absolute_error(preds_exp, y_exp))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(preds_exp, y_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012c073-fa2c-43cc-b0e5-6b4a082adc7f",
   "metadata": {},
   "source": [
    "The model makes mistakes at around 40%. For example, if ground truth value is 700, the model is likely to answer 980 or 420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b0f99-7a7b-473f-932b-8b0601cd4b2e",
   "metadata": {},
   "source": [
    "### Comparison to Ground Truth Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c8a33-9da8-4dd9-845c-4aa5c76aaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in random.sample(range(0, len(y_exp) - 1), 10):\n",
    "    print(\"Real: \", y_exp[i], \" Predicted: \", preds_exp[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0204e0-40b6-4554-81ec-71f5da736b97",
   "metadata": {},
   "source": [
    "### Testing Using Synthetic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880151bd-dc15-4e58-80cc-dca7243c9c9b",
   "metadata": {},
   "source": [
    "To imitate a real-world task a synthetic dataset is generated. It contains a row for each hour in a particular day for each station. The model should predict a duration of a trip, that starts from such station in such time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b560b4-daff-4422-8ce1-fe989c6e6564",
   "metadata": {},
   "source": [
    "Dataset generation and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4429f-486b-4ae0-9eca-476e752b780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {column_name: [] for column_name in X.columns}\n",
    "columns_from_X = [\n",
    "    \"Starting Station ID\",\n",
    "    \"Starting Station Latitude\",\n",
    "    \"Starting Station Longitude\",\n",
    "    \"Neighborhood Councils (Certified)\",\n",
    "    \"Council Districts\",\n",
    "    \"Zip Codes\",\n",
    "    \"Precinct Boundaries\",\n",
    "    \"Census Tracts\",\n",
    "]\n",
    "columns_same = {\n",
    "    \"Passholder Type\": \"Monthly Pass\",\n",
    "    \"Trip Route Category\": \"One Way\",\n",
    "    \"Plan Duration\": 30,\n",
    "    \"Year\": 2017,\n",
    "    \"Month\": 3,\n",
    "    \"Day\": 22,\n",
    "    \"Minute\": 0,\n",
    "}\n",
    "for i in range(118):  # there are 118 stations\n",
    "    for hour in range(24):\n",
    "        part_station = X[\n",
    "            X[\"Starting Station Latitude\"]\n",
    "            == start_stations.iloc[i][\"Starting Station Latitude\"]\n",
    "        ].reset_index(drop=True)\n",
    "        part_station_0 = part_station.iloc[0]\n",
    "        for column_name in columns_from_X:\n",
    "            d[column_name].append(part_station_0[column_name])\n",
    "        for column_name in columns_same:\n",
    "            d[column_name].append(columns_same[column_name])\n",
    "        d[\"Hour\"].append(hour)\n",
    "\n",
    "generated_ds = pd.DataFrame(d)\n",
    "generated_ds_encoded = count_encoder.transform(generated_ds)\n",
    "generated_ds_encoded[columns_to_normalize] = normalizer.transform(\n",
    "    generated_ds_encoded[columns_to_normalize]\n",
    ")\n",
    "generated_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298daa8b-a18e-4a7a-90a6-9e393958e810",
   "metadata": {},
   "source": [
    "Make predictions and unnormalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb3633-6985-4aa4-aaec-14bb45512933",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_generated = torch.tensor(generated_ds_encoded.to_numpy())\n",
    "tensor_generated_len = tensor_generated.shape[0]\n",
    "\n",
    "preds_for_gen_data = []\n",
    "\n",
    "for i in range(tensor_generated_len):\n",
    "    xb = tensor_generated[i].float()\n",
    "    pred = net(xb)\n",
    "    preds_for_gen_data.append(pred.item())\n",
    "\n",
    "unnormalized_preds_for_gen_data = normalizer_y.inverse_transform(\n",
    "    np.array(preds_for_gen_data).reshape(-1, 1)\n",
    ")\n",
    "preds_exp_for_gen_data = np.exp(unnormalized_preds_for_gen_data)\n",
    "\n",
    "generated_ds_preds = generated_ds[\n",
    "    [\"Starting Station Latitude\", \"Starting Station Longitude\", \"Hour\"]\n",
    "].copy()\n",
    "generated_ds_preds[\"Pred Duration\"] = preds_exp_for_gen_data\n",
    "\n",
    "dates = []\n",
    "for hour_value in generated_ds_preds[\"Hour\"]:\n",
    "    dates.append(str(pd.to_datetime(\"2017-03-22 \" + str(hour_value) + \":00:00\")))\n",
    "\n",
    "generated_ds_preds[\"Date\"] = dates\n",
    "\n",
    "generated_ds_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121dd94-664b-48fb-8e41-d096e92312cc",
   "metadata": {},
   "source": [
    "Points on a map represent starting stations location. The lighter a point, the longer a trip would start from it. There is also a time filter available. You can watch how trip duration estimation varies from time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_map = create_map()\n",
    "hourly_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97814dfe-ce17-4e29-a921-6b1e7faad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_dataset_id = str(uuid4())\n",
    "hourly_dataset_filter_id = str(uuid4())\n",
    "\n",
    "hourly_map.add_dataset(\n",
    "    id=hourly_dataset_id,\n",
    "    label=\"Hourly duration dataset\",\n",
    "    data=generated_ds_preds,\n",
    "    auto_create_layers=False,\n",
    ")\n",
    "\n",
    "hourly_map.add_filter(\n",
    "    id=hourly_dataset_filter_id,\n",
    "    sources=[\n",
    "        {\n",
    "            'data_id': hourly_dataset_id,\n",
    "            'field_name': 'Date'\n",
    "        }\n",
    "    ], \n",
    "    type=\"time-range\",\n",
    "    value=[1490140800000, 1490223600000],\n",
    ")\n",
    "\n",
    "hourly_map.add_layer(\n",
    "    {\n",
    "        \"id\": \"Hourly duration\",\n",
    "        \"type\": \"point\",\n",
    "        \"label\": \"Hourly duration\",\n",
    "        \"data_id\": hourly_dataset_id,\n",
    "        \"fields\": {\n",
    "            \"lat\": \"Starting Station Latitude\",\n",
    "            \"lng\": \"Starting Station Longitude\",\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"visual_channels\": {\n",
    "                \"colorScale\": \"quantize\",\n",
    "                \"colorField\": {\"name\": \"Pred Duration\", \"type\": \"float\"},\n",
    "            },\n",
    "            \"vis_config\": {\"radius\": 17},\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "hourly_map.set_view(longitude=-118.25, latitude=34.024, zoom=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
