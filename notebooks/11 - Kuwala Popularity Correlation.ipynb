{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Kuwala - Popularity Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notebook you can correlate any value associated with a geo-reference with the Google popularity score. You\n",
    "can upload your own file as a CSV. The only thing that is necessary to make it work is to have columns for latitude and\n",
    "longitude and column headers.\n",
    "\n",
    "The value columns can be specific to your use case, e.g., scooter bookings, sales in shops or crimes. The popularity\n",
    "score is aggregated on a week. So ideally, the value columns that you want to correlate are aggregated on a weekly\n",
    "timeframe as well.\n",
    "\n",
    "As an example we are using an open data set from Uber that gives us the traversals of rides through specific hexagons.\n",
    "You can find the raw data on their [open data platform](https://movement.uber.com/?lang=en-US). We preprocessed the raw\n",
    "data so that the traversals are already aggregated per week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Kuwala\n",
    "\n",
    "Kuwala is an open-source tool to build data products fast by integrating and connecting different data sources from\n",
    "various domains. Other pipelines than the popularity score include worldwide scalable POI and demographics data. Check\n",
    "out Kuwala on [GitHub](https://github.com/kuwala-io/kuwala) to access more data sources and functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set the file path to your CSV and the delimiter. Simply place your file under `notebooks/data` from within the\n",
    "Jupyter environment or on your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_path = './data/lisbon_uber_traversals.csv'\n",
    "delimiter = ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set the H3 resolution to aggregate the results on. To see the average size of a hexagon at a given resolution go to\n",
    "the [official H3 documentation](https://h3geo.org/docs/core-library/restable). The currently set resolution 8 has on\n",
    "average an edge length of 0.46 km which can be freely interpreted as a radius. For this example we are using the\n",
    "precalculated aggregations of the popularity score at resolution 8. To use a different resolution head over to\n",
    "[Kuwala's repository](https://github.com/kuwala-io/kuwala) to set up your data warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resolution = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set the column names for the coordinates and the columns of the file you want to correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lat_column = 'latitude'\n",
    "lng_column = 'longitude'\n",
    "value_columns = ['weekly_traversals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You can provide polygon coordinates as a GeoJSON-conforming array to select a subregion. Otherwise, data from the entire\n",
    "country of Portugal will be analyzed. (The default coordinates are a rough representation of Lisbon, Portugal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "polygon_coords = [[[-9.092559814453125,38.794500078219826],[-9.164314270019531,38.793429729760994],[-9.217529296875,38.76666579487878],[-9.216842651367188,38.68792166352608],[-9.12139892578125,38.70399894245585],[-9.0911865234375,38.74551518488265],[-9.092559814453125,38.794500078219826]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Load dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the file to correlate with the popularity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import h3\n",
    "import pandas as pd\n",
    "from geojson import Polygon\n",
    "\n",
    "\n",
    "def add_h3_index_column(row):\n",
    "    return h3.geo_to_h3(float(row[lat_column]), row[lng_column], resolution)\n",
    "\n",
    "\n",
    "def polyfill_polygon(poly):\n",
    "    h3_indexes = h3.polyfill(\n",
    "        dict(poly),\n",
    "        resolution,\n",
    "        geo_json_conformant=True\n",
    "    )\n",
    "\n",
    "    return h3_indexes\n",
    "\n",
    "df_file = pd.read_csv(file_path, sep=delimiter)\n",
    "df_file['h3_index'] = df_file.apply(add_h3_index_column, axis=1)\n",
    "\n",
    "if polygon_coords:\n",
    "    polygon = Polygon(polygon_coords)\n",
    "    h3_index_in_polygon = list(polyfill_polygon(poly=polygon))\n",
    "    bool_series = df_file.h3_index.isin(h3_index_in_polygon)\n",
    "    df_file = df_file[bool_series]\n",
    "\n",
    "df_file = df_file[['h3_index', *value_columns]].groupby(['h3_index']).sum()\n",
    "\n",
    "df_file.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get weekly popularity per hexagon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "popularity = pd.read_csv('./data/portugal_popularity.csv', sep=';')\n",
    "\n",
    "if polygon_coords:\n",
    "    polygon = Polygon(polygon_coords)\n",
    "    h3_index_in_polygon = list(polyfill_polygon(poly=polygon))\n",
    "    bool_series = popularity.h3_index.isin(h3_index_in_polygon)\n",
    "    popularity = popularity[bool_series]\n",
    "\n",
    "popularity.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = df_file.merge(popularity, on='h3_index', how='left')\n",
    "result[['weekly_popularity']] = result[['weekly_popularity']].fillna(value=0)\n",
    "\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(result, title=\"Pandas Profiling Report\", explorative=True)\n",
    "\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from unfolded.map_sdk import UnfoldedMap\n",
    "from sidecar import Sidecar\n",
    "from uuid import uuid4\n",
    "\n",
    "unfolded_map = UnfoldedMap()\n",
    "sc = Sidecar(title=f'Popularity Correlation', anchor='split-right')\n",
    "\n",
    "with sc:\n",
    "    display(unfolded_map)\n",
    "\n",
    "dataset_id_combined=uuid4()\n",
    "\n",
    "unfolded_map.add_dataset({\n",
    "    'uuid': dataset_id_combined,\n",
    "    'label': 'Correlated values',\n",
    "    'data': result\n",
    "})\n",
    "\n",
    "unfolded_map.add_layer({\n",
    "  'id': 'traversals',\n",
    "  'type': 'hexagonId',\n",
    "    'config': {\n",
    "    'data_id': dataset_id_combined,\n",
    "    \"label\": \"Traversals\",\n",
    "    'columns': {\n",
    "        'hex_id': 'h3_index'\n",
    "    },\n",
    "    'is_visible': True,\n",
    "    'color_scale': 'quantile',\n",
    "    'color_field': {\n",
    "      'name': 'weekly_traversals',\n",
    "      'type': 'real'\n",
    "    }\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
